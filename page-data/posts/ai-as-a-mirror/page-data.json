{"componentChunkName":"component---src-templates-post-template-post-template-tsx","path":"/posts/ai-as-a-mirror","result":{"data":{"markdownRemark":{"id":"440b2683-ddbe-59d1-b3bf-77eb512b953d","html":"<p>Foundation models based on neural networks were inspired by the the biological brain. Now, remarkably, their behaviour offers a mirror into the patterns of the brain.</p>\n<h3 id=\"scaling-laws\" style=\"position:relative;\"><a href=\"#scaling-laws\" aria-label=\"scaling laws permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Scaling laws</h3>\n<p>With small training data and model size, the GPT models only generate gibberish. However as the data and model size increases, they start getting a lot better.\nFirst generating random characters that look like words, then proper words but invalid sentences, then sentences that make sense and finally sentences that demonstrate reasoning.</p>\n<p>As anyone who has seen a baby grow from birth can tell, the parallels are striking.\nMuch like the LLMs, better training data (rich experiences and time spent talking to and interacting with) makes for better learning in babies.</p>\n<h3 id=\"biases\" style=\"position:relative;\"><a href=\"#biases\" aria-label=\"biases permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Biases</h3>\n<p>With LLMs, you can see biases based on the data it was trained on. Feed more right wing / left wing / choose your ideology, and that’s what the model mirrors.\nSimilarly, your perspectives, mannerisms, behaviour indisputably etch on the brains of your children and those around you.</p>\n<h3 id=\"backpropagation\" style=\"position:relative;\"><a href=\"#backpropagation\" aria-label=\"backpropagation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Backpropagation</h3>\n<p>Backprop is the key to neural networks. It is basically a feedback loop which updates the models weights until it is able to predict on training data correctly. Makes you think about the various feedback loops around you and the feedback loops you create.</p>\n<h3 id=\"learning-to-think\" style=\"position:relative;\"><a href=\"#learning-to-think\" aria-label=\"learning to think permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Learning to think</h3>\n<p><a href=\"https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Ted Chiang on a NewYorker piece</a> called ChatGPT a “blurry jpeg of the web” - basically a lossy compression of knowledge.\nInitial models like upto GPT 3.5 gave the impression that that even if these models had knowledge, they didn’t have wisdom.\nAs <a href=\"https://www.amazon.in/Siddhartha-Hermann-Hesse/dp/817234368X\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Siddhartha</a> says to his friend Govinda, “Knowledge can be transferred but not wisdom.”\nWisdom is application of knowledge in the right context. Each of our brains are wired differently and while you can regurgitate knowledge, the interpretation, application and the reasoning of that will vary from one individual to another because we all have different weights and biases and neural connections.\nYet, methods like Chain-of-thought prompting, RL fine tuning etc can teach to “think”, atleast impart patterns of thinking.\nReinforcement learning takes the same approach of how you have practice problems in text books after a few examples. The model is “rewarded” for getting the steps and finally the answer right. Because, until you are able to answer questions, you haven’t truly grokked the subject.</p>\n<h3 id=\"prediction-machines\" style=\"position:relative;\"><a href=\"#prediction-machines\" aria-label=\"prediction machines permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Prediction machines</h3>\n<p>LLMs were trivialised as next token predictors. It is after all how they work.\nBut the brain is similar in many ways. The brain is a prediction machine too. It is constantly trying to predict the state and anxiety is a state of not being able to predict well.</p>\n<blockquote>\n<p>Perception is less about passively receiving the world and more about actively predicting it. The brain’s goal? Minimize surprise. When it predicts well, you feel in control; when it can’t, things get shaky. <a href=\"https://www.theguardian.com/books/2021/aug/25/being-you-by-professor-anil-seth-review-the-exhilarating-new-science-of-consciousness\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Being You</a></p>\n</blockquote>\n<p>For e.g. we enjoy music because we are able to predict the next token in the rhythmic sequence.</p>\n<h3 id=\"language-as-the-key-to-thinking\" style=\"position:relative;\"><a href=\"#language-as-the-key-to-thinking\" aria-label=\"language as the key to thinking permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Language as the key to Thinking</h3>\n<p>Computers writing code or poetry or novels like humans was the last thing that perhaps people imagined in the AI roadmap.\nIt was far more realistic to imagine AIs solving for manual labor vs things that required higher order thinking.</p>\n<p>However, LLMs demonstrated that grokking language by learning through vast amounts of text, does unlock higher order thinking.</p>\n<p>Other species demonstrate reasoning abilities to solve problems but for higher order thinking, language seems to be the key and only humans have a true language center in the brain.</p>\n<p>There is a reason why GRE critical reasoning questions are in such a format.\nPerhaps having a rich vocabulary is key to have a higher level of perception.</p>\n<blockquote>\n<p>South Asian philosophical traditions suggest that language is fundamental to how we perceive the world, and that learning new words can even shape our experiences. <a href=\"https://www.bostonreview.net/articles/theres-a-word-for-that/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">There is a word for that</a></p>\n</blockquote>\n<h1 id=\"where-do-ai-models-go-from-here\" style=\"position:relative;\"><a href=\"#where-do-ai-models-go-from-here\" aria-label=\"where do ai models go from here permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Where do AI models go from here?</h1>\n<p>LLMs are already better than most humans in problem solving, writing, critical thinking etc.</p>\n<p>What are some interesting next steps?</p>\n<h2 id=\"agency\" style=\"position:relative;\"><a href=\"#agency\" aria-label=\"agency permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Agency</h2>\n<p>LLMs for better or worse do not have a concept of “self”. Currently, they are all but stateless token predictors.</p>\n<p>But add memory, self preservation and other goals, it can get interesting.</p>\n<blockquote>\n<p><a href=\"https://www.templeton.org/discoveries/agency-in-biology\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">What is Agency?</a></p>\n</blockquote>\n<h2 id=\"working-in-groups\" style=\"position:relative;\"><a href=\"#working-in-groups\" aria-label=\"working in groups permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Working in groups</h2>\n<p>It is fascinating that it is not Neanderthals, the sub species of Homo erectus which had bigger brains that ended up dominating but Homosapiens who apparently were in the Goldilock’s zone of “just right” social dynamics and brain size.</p>\n<p>Perhaps LLMs will also hit scaling limits where there is diminishing returns with model size and it is more efficient to be “distributed” and have different models collaborate.</p>","fields":{"slug":"/posts/2025-03-19--AI-As-A-Mirror//posts/ai-as-a-mirror","tagSlugs":["/tag/swdev/","/tag/reflection/"]},"frontmatter":{"date":"2025-03-19T09:44:32.169Z","description":"The foundation models offer a mirror to how we think","tags":["swdev","reflection"],"title":"AI as a mirror","socialImage":null}}},"pageContext":{"slug":"/posts/2025-03-19--AI-As-A-Mirror//posts/ai-as-a-mirror"}},"staticQueryHashes":["1943165318","251939775","401334301"]}